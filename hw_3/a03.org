#+title: A03

#+LATEX_HEADER: \setcounter{tocdepth}{1}

* Exercise 1
** Explain how the program is compiled and run.
Compilation is done with nvcc. Execution is done directly from the commandline.
#+begin_src
nvcc -arch=sm_61 -g main.cu -o main
./main 1024
#+end_src

** For a vector length of N:
*** How many floating operations are being performed in your vector add kernel?
$N$
*** How many global memory reads are being performed by your kernel?
$2N$. $5N$ in the unlikely case that blockIdx/blockDim/threadIdx are also stored in global memory.

** For a vector length of 1024:
*** Explain how many CUDA threads and thread blocks you used.
I used 32 threads per block and $(1024 + 32 - 1) / 32 = 32$ blocks. This results in $1024$ threads in total.
*** Profile your program with Nvidia Nsight. What Achieved Occupancy did you get? You might find https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-metric-comparison
I could not get nsight to work on Google Colab. Running the nsight-cli directly fails due to a missing metric. Selecting for specifically achieved occupancy fails with a nondescript error.
#+begin_src
!nv-nsight-cu-cli main 1024

The input length is 1024
==PROF== Connected to process 524 (/content/main)
Copying to device in 52 mus
==PROF== Profiling "vecAdd" - 1: 0%....50%....100% - 1 pass

==ERROR== LaunchFailed
Adding in 446948 mus
Copying from device in 12 mus
==PROF== Disconnected from process 524
==ERROR== An error occurred while trying to profile.
[524] main@127.0.0.1
  vecAdd(double*, double*, double*, int), 2022-Dec-12 19:35:32, Context 1, Stream 7
    Section: GPU Speed Of Light
    ---------------------------------------------------------------------- --------------- ------------------------------
    DRAM Frequency                                                                                                (!) n/a
    SM Frequency                                                                                                  (!) n/a
    Elapsed Cycles                                                                                                (!) n/a
    Memory [%]                                                                                                    (!) n/a
    SOL DRAM                                                                                                      (!) n/a
    Duration                                                                                                      (!) n/a
    SOL L1/TEX Cache                                                                                              (!) n/a
    SOL L2 Cache                                                                                                  (!) n/a
    SM Active Cycles                                                                                              (!) n/a
    SM [%]                                                                                                        (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule Bottleneck returned an error: Metric launch__waves_per_multiprocessor not found
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set
/root/Documents/NVIDIA Nsight
           Compute/2020.3.1/Sections/SpeedOfLight.py:45
/opt/nvidia/nsight-compute/2020.3.1/target/linux-desktop-glibc_2
          _11_3-x64/../../sections/NvRules.py:302

    Section: Launch Statistics
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Size                                                                                                    (!) n/a
    Function Cache Configuration                                                                                  (!) n/a
    Grid Size                                                                                                     (!) n/a
    Registers Per Thread                                                                                          (!) n/a
    Shared Memory Configuration Size                                                                              (!) n/a
    Driver Shared Memory Per Block                                                                                (!) n/a
    Dynamic Shared Memory Per Block                                                                               (!) n/a
    Static Shared Memory Per Block                                                                                (!) n/a
    Threads                                                                                                       (!) n/a
    Waves Per SM                                                                                                  (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
    ERR   Rule Launch Configuration returned an error: Metric launch__block_size not found
    ----- --------------------------------------------------------------------------------------------------------------
    ERR   <built-in function IAction_metric_by_name> returned a result with an error set
/root/Documents/NVIDIA Nsight
           Compute/2020.3.1/Sections/LaunchStatistics.py:45
/opt/nvidia/nsight-compute/2020.3.1/target/linux-desktop-gli
          bc_2_11_3-x64/../../sections/NvRules.py:302

    Section: Occupancy
    ---------------------------------------------------------------------- --------------- ------------------------------
    Block Limit SM                                                                                                (!) n/a
    Block Limit Registers                                                                                         (!) n/a
    Block Limit Shared Mem                                                                                        (!) n/a
    Block Limit Warps                                                                                             (!) n/a
    Theoretical Active Warps per SM                                                                               (!) n/a
    Theoretical Occupancy                                                                                         (!) n/a
    Achieved Occupancy                                                                                            (!) n/a
    Achieved Active Warps Per SM                                                                                  (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------
#+end_src

With only a specific metric:
#+begin_src
!nv-nsight-cu-cli --metrics sm__warps_active.avg.pct_of_peak_sustained_active main 1024

The input length is 1024
==PROF== Connected to process 572 (/content/main)
Copying to device in 49 mus
==PROF== Profiling "vecAdd" - 1: 0%....50%....100% - 1 pass

==ERROR== LaunchFailed
Adding in 364672 mus
Copying from device in 12 mus
==PROF== Disconnected from process 572
==ERROR== An error occurred while trying to profile.
[572] main@127.0.0.1
  vecAdd(double*, double*, double*, int), 2022-Dec-12 19:38:11, Context 1, Stream 7
    Section: Command line profiler metrics
    ---------------------------------------------------------------------- --------------- ------------------------------
    sm__warps_active.avg.pct_of_peak_sustained_active                                                             (!) n/a
    ---------------------------------------------------------------------- --------------- ------------------------------

#+end_src

In the end, I focussed on getting access to a desktop with a GPU (which tremendiously sped up development when compared to Google Colab). Unfortunately the GTX 1080, which I used, does not support profiling. So even this change did not allow me to use Nsight.
** Now increase the vector length to 131070:
*** Did your program still work? If not, what changes did you make?
It worked.
*** Explain how many CUDA threads and thread blocks you used.
The same as above, i.e. 32 threads per block and $131070$ threads. The number of blocks is thus $(131070 + 32 - 1) / 32 = 4096$.
*** Profile your program with Nvidia Nsight. What Achieved Occupancy do you get now?
Does not work due to the reason above.
** Further increase the vector length (try 6-10 different vector length), plot a stacked bar chart showing the breakdown of time including (1) data copy from host to device (2) the CUDA kernel (3) data copy from device to host. For this, you will need to add simple CPU timers to your code regions.
    | Input Length in Multiples of 131070 | To Device | Kernel | From Device |
    |-------------------------------------+-----------+--------+-------------|
    |                                   1 |      1027 | 157871 |           5 |
    |                                   2 |      1923 | 158321 |           5 |
    |                                   4 |      3807 | 157263 |           5 |
    |                                   8 |      7173 | 157361 |           4 |
    |                                  16 |     14262 | 155434 |           5 |
    |                                 128 |    115330 | 155241 |           5 |

    [[./ex_1/ex1.png]]

    The results seem unrealistic but as the time measurement seems correct to me and the result is correct, it is presumably correct due to optimizations in Cuda.


* Exercise 2
** Name three applications domains of matrix multiplication.
AI, computer graphics, solving linear equations (i.e. linear program optimization).
** How many floating operations are being performed in your matrix multiply kernel?Â 
numAColumns
** How many global memory reads are being performed by your kernel?
numAColumns * 2 + 1
** For a matrix A of (128x128) and B of (128x128):
*** Explain how many CUDA threads and thread blocks you used.Â 
32 threads per block and 128 * 128 / 32 blocks. 128 * 128 threads.
*** Profile your program with Nvidia Nsight. What Achieved Occupancy did you get?Â 
Does not work due to the reason above.
** For a matrix A of (511x1023) and B of (1023x4094):
*** Did your program still work? If not, what changes did you make?
Yes. At least, both CPU and GPU return the same results.
*** Explain how many CUDA threads and thread blocks you used.
511*4094 threads. 511*4094/32 blocks.
*** Profile your program with Nvidia Nsight. What Achieved Occupancy do you get now?
Does not work due to the reason above.
** Further increase the size of matrix A and B, plot a stacked bar chart showing the breakdown of time including (1) data copy from host to device (2) the CUDA kernel (3) data copy from device to host. For this, you will need to add simple CPU timers to your code regions. Explain what you observe.
[[./ex_2/double.png]]
The kernel execution time dominates for all input sizes which is natural as each cell in the output is the sum of thousands of multiplications. As a result, the memory size is negligible in relation to the number of computations.
** Now, change DataType from double to float, re-plot the a stacked bar chart showing the time breakdown. Explain what you observe.Â 
[[./ex_2/float.png]]
Using floats instead of doubles significantly speeds up the computation. Nevertheless, the speedup is significantly lower than might be expected. Although each float is only half the size of a double, its multiplication is almost as expensive. As the multiplications dominate the execution time the impact as not that large in the end.

* Exercise 3
** Describe all optimizations you tried regardless of whether you committed to them or abandoned them and whether they improved or hurt performance.Â 
- Adapt blocksize to the problem. 4096 for saturation, size of input for the counting.
- Have a thread for every bin instead of for every input. This would require only 4096 threads but no atomics would be needed.
- Use shared memory.
** Which optimizations you chose in the end and why?Â 
Only the first one. Having only 4096 threads iterating the whole input decreases parallelism significantly. Looping is also not as efficient on GPUs due to lacking a branch predictor.

Shared memory was not used as it is not obvious how this would be done. Each part of the input may contain any integer, i.e. be related to any bin. Therefore, putting the bins into the shared memory is not easily possible. Having a copy of all the bins for each block (and adding the results of each block afterwards) was assumed to be slower as this multiplies the amount of required memory.
** How many global memory reads are being performed by your kernel? ExplainÂ 
Two for every execution of the counting. Once to read the input value, once to read the value from the bin before adding one.

One for every execution of the saturation. A bins value is checked and, if larger than 127, set to 127.

** How many atomic operations are being performed by your kernel? Explain
One for each input. We add 1 to the value of the bin.

** How much shared memory is used in your code? Explain
None. Instead we use the global memory directly.

** How would the value distribution of the input array affect the contention among threads? For instance, what contentions would you expect if every element in the array has the same value?Â 
The value distribution affects the contention significantly. If each input was the same value, we would expect almost 100% contention as each thread atomically adds to the same bin.

** Plot a histogram generated by your code and specify your input length, thread block and grid.
[[./ex_3/histo.png]]

This histogram has been generated using an input length of 100000 (larger values saturate all bins). Note that the graphing done with matplotlib shows some artifacts for higher values. The distance between the bars bears no significance.

I kept a thread per block number of 32 and created one thread for every input. This results in $100000 / 32 = 3125$ blocks.

** For a input array of 1024 elements, profile with Nvidia Nsight and report Shared Memory Configuration Size and Achieved Occupancy. Did Nvsight report any potential performance issues?
Does not work due to the reason above.

* Exercise 4
** Describe the environment you used, what changes you made to the Makefile, and how you ran the simulation.
GTX 1080 on a Linux desktop running openSUSE Tumbleweed. The architecture was set to sm_62. Besides that, no changes where made to the Makefile. It was executed by simply passing the input file to the program.
** Describe your design of the GPU implementation of mover_PC() briefly.Â 
We define a wrapper mover_PC_gpu and a kernel mover_PC_kernel. One thread executing the kernel handles moving one particle, i.e. one iteration of the "Move each particle with new fields" loop. Subcycling is handled by the wrapper.

The wrapper allocates all of the memory by first creating copies on the host, replacing the pointers with device memory and then copying this to the device. We use exclusively flat arrays.
** Compare the output of both CPU and GPU implementation to guarantee that your GPU implementations produce correct answers.
When viewed through paraview, the rho_net files looked identical. I assumed that this was sufficiently accurate.

** Compare the execution time of your GPU implementation with its CPU version.
The Mover Time / Cycle is 0.136 for the GPU version of the program. The CPU version has about 1.7, i.e. a ten times larger value.
